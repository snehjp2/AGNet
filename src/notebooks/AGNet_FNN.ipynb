{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" neural network \"\"\"\n",
    "# importing packages\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from pickle import dump, load\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.decomposition import PCA\n",
    "import tqdm\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original matched catalog had ~29k AGN.  Filtering out errors > .4 dex brings catalog to ~23,000 AGN.  I find that filtering out errors >.3 dex gives best performance.  This brings full catalog to 20,549 AGN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Black Hole dataset--default parameters are for training and predicting AGN mass.  Pass 'train=False' \n",
    "# for test-set and 'mass=False' for AGN redshift prediction.\n",
    "class BHDataset(Dataset):\n",
    "    def __init__(self, path, train=True, mass=True):\n",
    "        self.path = path\n",
    "        self.train = train\n",
    "        self.mass = mass\n",
    "        self.sc = StandardScaler()\n",
    "        self.pca = PCA(.95)\n",
    "        \n",
    "        if self.mass:\n",
    "            \n",
    "            if self.train:\n",
    "                self.data = pd.read_csv(self.path + 'TRAIN_dr14.csv')\n",
    "                self.features = self.sc.fit_transform(np.asarray(self.data.iloc[:,[6,8,9,10,11,12,13,14,15,16,17,18]]))\n",
    "                self.pca.fit(self.features)\n",
    "                self.features = self.pca.transform(self.features)\n",
    "                dump(self.sc, open('train_scaler.pkl','wb'))\n",
    "                dump(self.pca, open('train_scaler_pca.pkl','wb'))\n",
    "        \n",
    "            else:\n",
    "                self.data = pd.read_csv(self.path + 'TEST_dr14.csv')\n",
    "                self.sc = load(open('train_scaler.pkl','rb'))\n",
    "                self.pca = load(open('train_scaler_pca.pkl','rb'))\n",
    "                self.features = self.sc.transform(np.asarray(self.data.iloc[:,[6,8,9,10,11,12,13,14,15,16,17,18]]))\n",
    "                self.features = self.pca.transform(self.features)\n",
    "        else:\n",
    "            \n",
    "            if self.train:\n",
    "                self.data = pd.read_csv(self.path + 'TRAIN_dr14.csv')\n",
    "                self.features = self.sc.fit_transform(np.asarray(self.data.iloc[:,9:]))\n",
    "                self.pca.fit(self.features)\n",
    "                self.features = self.pca.transform(self.features)\n",
    "                dump(self.sc, open('train_scaler.pkl','wb'))\n",
    "                dump(self.pca, open('train_scaler_pca.pkl','wb'))\n",
    "        \n",
    "            else:\n",
    "                self.data = pd.read_csv(self.path + 'TEST_dr14.csv')\n",
    "                self.sc = load(open('train_scaler.pkl','rb'))\n",
    "                self.pca = load(open('train_scaler_pca.pkl','rb'))\n",
    "                self.features = self.sc.transform(np.asarray(self.data.iloc[:,9:]))\n",
    "                self.features=(self.pca.transform(self.features))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        if self.mass:\n",
    "            \n",
    "            ID = torch.from_numpy(np.asarray(self.data.iloc[index,0]))\n",
    "            target = torch.from_numpy(np.asarray(self.data.iloc[index,5]))\n",
    "            features = torch.from_numpy(self.features[index].reshape(1,-1).squeeze())\n",
    "            error = torch.from_numpy(np.asarray(self.data.iloc[index,7]))\n",
    "            return (ID, features, target, error)\n",
    "        \n",
    "        else:\n",
    "            ID = torch.from_numpy(np.asarray(self.data.iloc[index,0]))\n",
    "            target = torch.from_numpy(np.asarray(self.data.iloc[index,6]))\n",
    "            features = torch.from_numpy(self.features[index].reshape(1,-1).squeeze())\n",
    "            return (ID, features, target)\n",
    "\n",
    "        \n",
    "# define train and test datasets.  Train test split was done previously using sklearn.\n",
    "train_mass = BHDataset('../../data/dr14/')\n",
    "test_mass = BHDataset('../../data/dr14/', train=False)\n",
    "\n",
    "train_z = BHDataset('../../data/dr14/', mass=False)\n",
    "test_z = BHDataset('../../data/dr14/', mass=False, train=False)\n",
    "train_mass.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA reduces dimensionality of AGN BH Mass prediction from 12 features --> 6 features.  PCA reduces dimensionality of AGN redshift prediction from 10 features --> 5 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataloaders with the datasets.  Only shuffle training sets.\n",
    "train_dl_mass = DataLoader(train_mass, batch_size=256, shuffle=True)\n",
    "test_dl_mass = DataLoader(test_mass, batch_size=256, shuffle=False)\n",
    "\n",
    "train_dl_z = DataLoader(train_z, batch_size=256, shuffle=True)\n",
    "test_dl_z = DataLoader(test_z, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default architecture is to predict AGN mass.  Pass 'mass=False' to predict redshift.\n",
    "class AGNet(nn.Module):\n",
    "    def __init__(self, mass=True):\n",
    "        super().__init__()\n",
    "        self.mass = mass\n",
    "        \n",
    "        if self.mass:\n",
    "\n",
    "            self.fc1 = nn.Linear(6, 32)\n",
    "            self.fc2 = nn.Linear(32, 64)\n",
    "            self.fc3 = nn.Linear(64, 128)\n",
    "            self.fc4 = nn.Linear(128, 128)\n",
    "            self.fc5 = nn.Linear(128, 64)\n",
    "            self.fc6 = nn.Linear(64, 32)\n",
    "            self.fc7 = nn.Linear(32, 1)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.fc1 = nn.Linear(5, 32)\n",
    "            self.fc2 = nn.Linear(32, 64)\n",
    "            self.fc3 = nn.Linear(64, 128)\n",
    "            self.fc4 = nn.Linear(128, 128)\n",
    "            self.fc5 = nn.Linear(128, 64)\n",
    "            self.fc6 = nn.Linear(64, 32)\n",
    "            self.fc7 = nn.Linear(32, 1)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = self.fc7(x)\n",
    "        return x\n",
    "\n",
    "# defining neural networks\n",
    "net_mass = AGNet()\n",
    "net_z = AGNet(mass=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count parameters for reference.  For both redshift and mass, (training size/model parameters) < 1.\n",
    "(count_parameters(net_mass), count_parameters(net_z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mse_loss(mu, target, var):\n",
    "    return torch.mean(var**-1 * (mu - target) ** 2 + var)\n",
    "\n",
    "def weighted_smoothl1_loss(mu, target, var):\n",
    "    if torch.abs(torch.mean(mu) - torch.mean(target)) < 1:\n",
    "        loss = torch.mean(0.5*(mu - target)^2 + var)\n",
    "    else:\n",
    "        loss = torch.mean(torch.abs(mu - target) - 0.5 + var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining optimizer and loss function\n",
    "lr = .005 # .01 for redshift\n",
    "optimizer = optim.AdamW(net_z.parameters(), lr=lr)\n",
    "loss_function = nn.SmoothL1Loss() #for mass, nn.MSELoss() for redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop takes in epoch #, network, train loader, loss function, and optimizer.  \n",
    "def train(num_epochs, trainloader, testloader, mdl, batch_size=256):\n",
    "    \n",
    "    best_rmse = float('inf')\n",
    "    epoch_list = np.linspace(1, num_epochs , num = num_epochs)\n",
    "    train_loss_list, train_rmse_list, test_loss_list, test_rmse_list = list(), list(), list(), list()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "            train_pred, train_gt, test_pred, test_gt = list(), list(), list(), list()\n",
    "\n",
    "            for data in trainloader:\n",
    "\n",
    "                ID, features, ground_truth= data\n",
    "                train_gt.append(ground_truth.float())\n",
    "                output_train = mdl(features.float())\n",
    "                train_pred.append(output_train.float())\n",
    "                train_loss = loss_function(output_train.squeeze(), ground_truth.float().squeeze())\n",
    "                mdl.zero_grad()\n",
    "                train_loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            train_loss_list.append(train_loss.item()/batch_size)\n",
    "            train_ground_truth = torch.cat(train_gt).data\n",
    "            train_predictions = torch.cat(train_pred).data.flatten()\n",
    "            train_rmse = np.sqrt(metrics.mean_squared_error(train_ground_truth, train_predictions))\n",
    "            train_rmse_list.append(train_rmse)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                for data_ in testloader:  \n",
    "\n",
    "                    ID_, features_, ground_truth_ = data_  \n",
    "                    test_gt.append(ground_truth_.float())\n",
    "                    output_test = mdl(features_.float()) \n",
    "                    test_pred.append(output_test.float())\n",
    "                    test_loss = loss_function(output_test.squeeze(), ground_truth_.squeeze()) # was.float() before\n",
    "                    \n",
    "                test_loss_list.append(test_loss.item()/batch_size)\n",
    "                test_ground_truth = torch.cat(test_gt).data.flatten()\n",
    "                test_predictions = torch.cat(test_pred).data.flatten()\n",
    "                test_rmse = np.sqrt(metrics.mean_squared_error(test_ground_truth, test_predictions))\n",
    "                test_rmse_list.append(test_rmse)\n",
    "                \n",
    "                if test_rmse < best_rmse:\n",
    "                    best_rmse = test_rmse\n",
    "                    datetime_today = str(datetime.date.today())\n",
    "                    torch.save(net_mass, '../saved_models/' + datetime_today + 'sneh' + str(best_rmse) + 'mlp.mdl')\n",
    "                    print(\"saved to \" + \"sneh_mlp.mdl\" + \" file.\")\n",
    "                    \n",
    "                print(f'EPOCH: {epoch+1}\\nTrain Loss: {train_loss.item():.5f} | Train RMSE: {train_rmse:.5f}\\nTest Loss: {test_loss.item():.5f} | Test RMSE: {test_rmse:.5f}')\n",
    "            \n",
    "    return epoch_list, train_loss_list, test_loss_list, train_rmse_list, test_rmse_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(epoch_list, train_loss, test_loss, train_rmse, test_rmse):\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "#     ax1.set_xticks(len(epoch_list)/5)\n",
    "#     ax2.set_xticks(len(epoch_list)/5)\n",
    "    fig.set_figheight(10)\n",
    "    fig.set_figwidth(15)\n",
    "    \n",
    "    fig.suptitle('Loss and RMSE Curves', fontsize=16)\n",
    "    \n",
    "    ax1.set(xlabel = 'EPOCH', ylabel = 'LOSS')\n",
    "    ax1.plot(epoch_list, train_loss, label = 'Train Loss', color = 'blue')\n",
    "    ax1.plot(epoch_list, test_loss, label = 'Test Loss', color = 'orange', ls='--')\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.set(xlabel = 'EPOCH', ylabel = 'RMSE')\n",
    "    ax2.set_ylim([0,1])\n",
    "    ax2.plot(epoch_list, train_rmse, label = 'Train RMSE', color = 'blue')\n",
    "    ax2.plot(epoch_list, test_rmse, label = 'Test RMSE', color = 'orange', ls='--') \n",
    "    ax2.legend()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must change train function parameters depending on mass or z training\n",
    "epoch_list, train_loss, test_loss, train_rmse, test_rmse = train(100, train_dl_z, test_dl_z, net_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will return loss vs. epoch and RMSE vs. epoch curves\n",
    "plot_loss_curve(epoch_list, train_loss, test_loss, train_rmse, test_rmse)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current combination of number of epochs, loss function, and optimizer are what we found to work best.  We encourage users to explore other combinations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# torch.save(net_mass.state_dict(), '../../models/AGNet_dr14_6features_lr.005_50epoch.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test loop outputs plot of results + dataframe of object ID, ground truth values, and network predictions\n",
    "def test(testloader, mdl, df=False):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        test_ID, test_pred, test_gt = list(), list(), list()\n",
    "\n",
    "        for data in testloader:\n",
    "            ID, features, ground_truth = data  \n",
    "            test_ID.append(ID.float())\n",
    "            test_gt.append(ground_truth.float())\n",
    "            output_test = mdl(features.float()) \n",
    "            test_pred.append(output_test.float())\n",
    "            test_loss = loss_function(output_test.squeeze(), ground_truth.squeeze())\n",
    "\n",
    "        test_ground_truth = torch.cat(test_gt).data\n",
    "        test_predictions = torch.cat(test_pred).data.flatten()\n",
    "        test_rmse = np.sqrt(metrics.mean_squared_error(test_ground_truth, test_predictions))\n",
    "        ID = torch.cat(test_ID).data\n",
    "        \n",
    "        plt.figure(figsize=[8,8])\n",
    "        plt.plot(test_ground_truth, test_ground_truth,color='black', label = 'Mass Ground Truth')\n",
    "        plt.scatter(test_ground_truth, test_predictions,s=2, color='blue', label = 'NN prediction')\n",
    "        plt.title('RMSE:' + str(test_rmse) + ' | R2: ' + str(metrics.r2_score(test_ground_truth,test_predictions)) + '| Pearson:' + str(pearsonr(test_ground_truth,test_predictions)[0]))\n",
    "        plt.xlabel('AGN Mass')\n",
    "        plt.ylabel('AGN Mass')\n",
    "#         plt.savefig('/Users/SnehPandya/Desktop/plot.png', facecolor='white')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    if df: \n",
    "        \n",
    "        results = pd.DataFrame({'my_ID':ID.numpy().astype('int'), 'ground_truth':test_ground_truth.numpy(), 'network_predictions':test_predictions.numpy() })\n",
    "        return results\n",
    "    \n",
    "    else:\n",
    "        print('pass df=True to create dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test(test_dl_mass, net_mass, df=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
